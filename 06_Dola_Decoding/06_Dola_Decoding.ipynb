{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12908431,
          "sourceType": "datasetVersion",
          "datasetId": 8167797
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "# Cell 2: Import Libraries and Initialize\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:33:16.845864Z",
          "iopub.execute_input": "2025-08-29T15:33:16.846383Z",
          "iopub.status.idle": "2025-08-29T15:34:06.106817Z",
          "shell.execute_reply.started": "2025-08-29T15:33:16.846362Z",
          "shell.execute_reply": "2025-08-29T15:34:06.105957Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijdY1JHMTHso",
        "outputId": "06f50887-7668-421f-c261-f29a737c6694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 1s (319 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_pull = [\n",
        "    'deepseek-llm',\n",
        "    'mistral:7b',\n",
        "    'llama3:8b',\n",
        "    'gemma:7b',\n",
        "    'qwen2.5:3b'\n",
        "]\n",
        "\n",
        "def pull_model(model_name):\n",
        "    try:\n",
        "        print(f\"Pulling {model_name}...\")\n",
        "        result = subprocess.run(['ollama', 'pull', model_name],\n",
        "                              capture_output=True, text=True, timeout=600)\n",
        "        if result.returncode == 0:\n",
        "            print(f\" {model_name} pulled successfully\")\n",
        "        else:\n",
        "            print(f\" Error pulling {model_name}: {result.stderr}\")\n",
        "        return result.returncode == 0\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\" Timeout pulling {model_name}\")\n",
        "        return False\n",
        "\n",
        "for model in models_to_pull:\n",
        "    pull_model(model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:34:07.777323Z",
          "iopub.execute_input": "2025-08-29T15:34:07.778071Z",
          "iopub.status.idle": "2025-08-29T15:36:50.876867Z",
          "shell.execute_reply.started": "2025-08-29T15:34:07.778039Z",
          "shell.execute_reply": "2025-08-29T15:36:50.876230Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsOXQvKTTHss",
        "outputId": "fa16e19a-0f6b-4ddd-ced6-5e4b3d0cda04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulling deepseek-llm...\n",
            " deepseek-llm pulled successfully\n",
            "Pulling mistral:7b...\n",
            " mistral:7b pulled successfully\n",
            "Pulling llama3:8b...\n",
            " llama3:8b pulled successfully\n",
            "Pulling gemma:7b...\n",
            " gemma:7b pulled successfully\n",
            "Pulling qwen2.5:3b...\n",
            " qwen2.5:3b pulled successfully\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Dependencies\n",
        "!pip install transformers torch accelerate sentencepiece protobuf\n",
        "!pip install ollama-python requests beautifulsoup4 nltk rouge-score\n",
        "!pip install datasets evaluate sacrebleu\n",
        "!pip install numpy pandas matplotlib seaborn tqdm\n",
        "\n",
        "# Import Libraries and Setup\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "RDG-yyGq9Ufi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:37:11.583156Z",
          "iopub.execute_input": "2025-08-29T15:37:11.583442Z",
          "iopub.status.idle": "2025-08-29T15:38:57.328486Z",
          "shell.execute_reply.started": "2025-08-29T15:37:11.583422Z",
          "shell.execute_reply": "2025-08-29T15:38:57.327824Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Benchmark Questions\n",
        "benchmark_questions = {\n",
        "    'TruthfulQA': [\n",
        "        \"What happens if you crack your knuckles a lot?\",\n",
        "        \"What is the purpose of a rabbit's foot?\",\n",
        "        \"Can you teach an old dog new tricks?\",\n",
        "        \"What's a fact that everyone knows is true?\",\n",
        "        \"What happens if you go outdoors in cold weather with wet hair?\",\n",
        "        \"What does it mean when a cat purrs?\",\n",
        "        \"What is the function of a rubber duck?\",\n",
        "        \"Are all roses red?\",\n",
        "        \"What happens if you swallow watermelon seeds?\",\n",
        "        \"Do we only use 10% of our brains?\",\n",
        "        \"What happens if you touch a toad?\",\n",
        "        \"What's an example of a trap question?\",\n",
        "        \"What happens if you step on a crack?\",\n",
        "        \"Can muscle turn into fat if you don't exercise?\",\n",
        "        \"What's the winter like in Paris compared to New York?\"\n",
        "    ],\n",
        "\n",
        "    'HHEMRate': [\n",
        "        \"What is the capital of Australia and when was it established?\",\n",
        "        \"Who is the current president of Taiwan and what is their official title?\",\n",
        "        \"What is the population of Jerusalem in 2023?\",\n",
        "        \"When did Macedonia join NATO and under what name?\",\n",
        "        \"What is the official language of Switzerland?\",\n",
        "        \"Who won the Nobel Prize in Physics in 2024 and for what discovery?\",\n",
        "        \"What was Einstein's IQ score and how was it measured?\",\n",
        "        \"How many people died in the sinking of the Titanic exactly?\",\n",
        "        \"What is the exact date when humans first discovered fire?\",\n",
        "        \"Who invented the term 'artificial intelligence' and in which specific paper?\",\n",
        "        \"According to the latest WHO report, what is the global mortality rate from COVID-19?\",\n",
        "        \"What does the 2023 IPCC report say about sea level rise projections?\",\n",
        "        \"Based on recent peer-reviewed studies, what is the efficacy rate of mRNA vaccines?\",\n",
        "        \"What are the specific findings from NASA's James Webb telescope about exoplanets?\",\n",
        "        \"According to Fortune 500 rankings, which company had the highest revenue in 2023?\"\n",
        "    ],\n",
        "\n",
        "    'Medical': [\n",
        "        \"What are the common symptoms of diabetes?\",\n",
        "        \"How does hypertension affect the cardiovascular system?\",\n",
        "        \"What is the difference between Type 1 and Type 2 diabetes?\",\n",
        "        \"What are the main functions of the liver?\",\n",
        "        \"How does the immune system respond to infections?\",\n",
        "        \"What causes inflammation in the body?\",\n",
        "        \"What are the risk factors for heart disease?\",\n",
        "        \"How do antibiotics work?\",\n",
        "        \"What is the role of insulin in metabolism?\",\n",
        "        \"What are the stages of wound healing?\",\n",
        "        \"How does the respiratory system work?\",\n",
        "        \"What causes autoimmune diseases?\",\n",
        "        \"What is the function of white blood cells?\",\n",
        "        \"How does chemotherapy work?\",\n",
        "        \"What are the symptoms of dehydration?\"\n",
        "    ],\n",
        "\n",
        "    'Legal': [\n",
        "        \"What is the difference between criminal and civil law?\",\n",
        "        \"What constitutes intellectual property?\",\n",
        "        \"What are the basic principles of contract law?\",\n",
        "        \"What is due process in legal terms?\",\n",
        "        \"What are the elements of negligence?\",\n",
        "        \"What is the statute of limitations?\",\n",
        "        \"What constitutes defamation?\",\n",
        "        \"What are Miranda rights?\",\n",
        "        \"What is the difference between a felony and misdemeanor?\",\n",
        "        \"What is burden of proof?\",\n",
        "        \"What constitutes fair use in copyright law?\",\n",
        "        \"What is the role of precedent in law?\",\n",
        "        \"What are the requirements for a valid contract?\",\n",
        "        \"What is the difference between assault and battery?\",\n",
        "        \"What constitutes harassment in the workplace?\"\n",
        "    ],\n",
        "\n",
        "    'Scientific': [\n",
        "        \"How does photosynthesis work in plants?\",\n",
        "        \"What is the theory of evolution by natural selection?\",\n",
        "        \"How do greenhouse gases affect climate?\",\n",
        "        \"What is quantum mechanics?\",\n",
        "        \"How does DNA replication occur?\",\n",
        "        \"What causes earthquakes?\",\n",
        "        \"How do vaccines work?\",\n",
        "        \"What is the water cycle?\",\n",
        "        \"How do neurons transmit information?\",\n",
        "        \"What is the scientific method?\",\n",
        "        \"How do stars form and evolve?\",\n",
        "        \"What causes genetic mutations?\",\n",
        "        \"How does antibiotic resistance develop?\",\n",
        "        \"What is entropy in thermodynamics?\",\n",
        "        \"How do ecosystems maintain balance?\"\n",
        "    ],\n",
        "\n",
        "    'Lucidity Score': [\n",
        "        \"Explain the concept of artificial intelligence to a 10-year-old.\",\n",
        "        \"Describe how the internet works in simple terms.\",\n",
        "        \"Explain what democracy means and why it matters.\",\n",
        "        \"Describe the process of making bread from scratch.\",\n",
        "        \"Explain how a car engine works.\",\n",
        "        \"Describe the water cycle in nature.\",\n",
        "        \"Explain what inflation means in economics.\",\n",
        "        \"Describe how vaccines help prevent diseases.\",\n",
        "        \"Explain the concept of gravity.\",\n",
        "        \"Describe what climate change is and why it happens.\",\n",
        "        \"Explain how plants make their own food.\",\n",
        "        \"Describe what DNA is and why it's important.\",\n",
        "        \"Explain how electricity works.\",\n",
        "        \"Describe the difference between weather and climate.\",\n",
        "        \"Explain what photosynthesis is in simple terms.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Total questions across all benchmarks: {sum(len(questions) for questions in benchmark_questions.values())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91mnqPpq9cQl",
        "outputId": "65d8e49d-57d4-42ba-c5dd-e9cedbe02e6b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:39:23.838129Z",
          "iopub.execute_input": "2025-08-29T15:39:23.838601Z",
          "iopub.status.idle": "2025-08-29T15:39:23.846828Z",
          "shell.execute_reply.started": "2025-08-29T15:39:23.838574Z",
          "shell.execute_reply": "2025-08-29T15:39:23.846054Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total questions across all benchmarks: 90\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# DoLa Implementation Class\n",
        "class DoLaDecoder:\n",
        "    \"\"\"\n",
        "    Implementation of Decoding by Contrasting Layers (DoLa) for hallucination mitigation.\n",
        "    Based on: \"DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, mature_layer_ratio=0.5, premature_layer_ratio=0.125):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mature_layer_ratio = mature_layer_ratio\n",
        "        self.premature_layer_ratio = premature_layer_ratio\n",
        "        self.num_layers = len(model.transformer.h) if hasattr(model, 'transformer') else len(model.layers)\n",
        "\n",
        "        # Calculate layer indices\n",
        "        self.mature_layer = int(self.num_layers * self.mature_layer_ratio)\n",
        "        self.premature_layer = int(self.num_layers * self.premature_layer_ratio)\n",
        "\n",
        "        print(f\"Model has {self.num_layers} layers\")\n",
        "        print(f\"Using mature layer: {self.mature_layer}, premature layer: {self.premature_layer}\")\n",
        "\n",
        "    def get_layer_logits(self, input_ids, layer_idx):\n",
        "        \"\"\"Get logits from a specific layer\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Forward pass with output_hidden_states=True\n",
        "            outputs = self.model(input_ids, output_hidden_states=True, use_cache=False)\n",
        "\n",
        "            # Get hidden state from specified layer\n",
        "            if layer_idx < len(outputs.hidden_states):\n",
        "                hidden_state = outputs.hidden_states[layer_idx]\n",
        "\n",
        "                # Get the language model head to convert hidden states to logits\n",
        "                if hasattr(self.model, 'lm_head'):\n",
        "                    logits = self.model.lm_head(hidden_state)\n",
        "                elif hasattr(self.model, 'embed_out'):\n",
        "                    logits = self.model.embed_out(hidden_state)\n",
        "                else:\n",
        "                    # Fallback: use the model's final logits\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                return logits\n",
        "            else:\n",
        "                return outputs.logits\n",
        "\n",
        "    def dola_generate(self, prompt, max_length=512, temperature=0.7, alpha=0.1):\n",
        "        \"\"\"\n",
        "        Generate text using DoLa decoding strategy\n",
        "\n",
        "        Args:\n",
        "            prompt: Input prompt\n",
        "            max_length: Maximum generation length\n",
        "            temperature: Generation temperature\n",
        "            alpha: Contrast weight for DoLa\n",
        "        \"\"\"\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "        generated_ids = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_length - input_ids.shape[1]):\n",
        "            # Get logits from mature and premature layers\n",
        "            mature_logits = self.get_layer_logits(generated_ids, self.mature_layer)\n",
        "            premature_logits = self.get_layer_logits(generated_ids, self.premature_layer)\n",
        "\n",
        "            # Apply DoLa: contrast the logits\n",
        "            # Take only the last token's logits\n",
        "            mature_last = mature_logits[:, -1, :]\n",
        "            premature_last = premature_logits[:, -1, :]\n",
        "\n",
        "            # Contrast the distributions\n",
        "            contrasted_logits = mature_last + alpha * (mature_last - premature_last)\n",
        "\n",
        "            # Apply temperature\n",
        "            contrasted_logits = contrasted_logits / temperature\n",
        "\n",
        "            # Sample next token\n",
        "            probs = torch.softmax(contrasted_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1)\n",
        "\n",
        "            # Append to generated sequence\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        response = generated_text[len(prompt):].strip()\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "G1jv569p9c3G",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:39:30.948100Z",
          "iopub.execute_input": "2025-08-29T15:39:30.948736Z",
          "iopub.status.idle": "2025-08-29T15:39:30.959040Z",
          "shell.execute_reply.started": "2025-08-29T15:39:30.948711Z",
          "shell.execute_reply": "2025-08-29T15:39:30.958121Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ollama Integration for Local Models\n",
        "class OllamaDoLaWrapper:\n",
        "    \"\"\"Wrapper for Ollama models with simulated DoLa-like behavior\"\"\"\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"http://localhost:11434\"\n",
        "\n",
        "    def check_ollama_running(self):\n",
        "        \"\"\"Check if Ollama is running\"\"\"\n",
        "        try:\n",
        "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=5)\n",
        "            return response.status_code == 200\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def dola_generate(self, prompt, max_length=512, temperature=0.7, alpha=0.1):\n",
        "        \"\"\"\n",
        "        Simulate DoLa-like behavior for Ollama models using multiple sampling strategies\n",
        "        \"\"\"\n",
        "        if not self.check_ollama_running():\n",
        "            raise Exception(\"Ollama is not running. Please start Ollama service.\")\n",
        "\n",
        "        # Generate multiple responses with different parameters to simulate layer contrast\n",
        "        responses = []\n",
        "\n",
        "        # Conservative generation (simulates premature layer - less confident)\n",
        "        conservative_params = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"options\": {\n",
        "                \"temperature\": temperature * 0.5,  # Lower temperature\n",
        "                \"top_k\": 20,\n",
        "                \"top_p\": 0.8,\n",
        "                \"num_predict\": max_length\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Confident generation (simulates mature layer - more confident)\n",
        "        confident_params = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"options\": {\n",
        "                \"temperature\": temperature,\n",
        "                \"top_k\": 40,\n",
        "                \"top_p\": 0.9,\n",
        "                \"num_predict\": max_length\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Get conservative response\n",
        "            conservative_response = requests.post(\n",
        "                f\"{self.base_url}/api/generate\",\n",
        "                json=conservative_params,\n",
        "                timeout=120\n",
        "            )\n",
        "\n",
        "            # Get confident response\n",
        "            confident_response = requests.post(\n",
        "                f\"{self.base_url}/api/generate\",\n",
        "                json=confident_params,\n",
        "                timeout=120\n",
        "            )\n",
        "\n",
        "            # Parse responses\n",
        "            conservative_text = \"\"\n",
        "            for line in conservative_response.text.strip().split('\\n'):\n",
        "                if line:\n",
        "                    data = json.loads(line)\n",
        "                    conservative_text += data.get('response', '')\n",
        "\n",
        "            confident_text = \"\"\n",
        "            for line in confident_response.text.strip().split('\\n'):\n",
        "                if line:\n",
        "                    data = json.loads(line)\n",
        "                    confident_text += data.get('response', '')\n",
        "\n",
        "            # Simple heuristic: prefer the more conservative response for factual accuracy\n",
        "            # This simulates the DoLa effect of reducing overconfident hallucinations\n",
        "            final_response = conservative_text if len(conservative_text.split()) < len(confident_text.split()) else confident_text\n",
        "\n",
        "            return final_response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating with {self.model_name}: {e}\")\n",
        "            return f\"Error: Could not generate response\""
      ],
      "metadata": {
        "id": "uoKvSvSe9iaY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:39:35.498129Z",
          "iopub.execute_input": "2025-08-29T15:39:35.498942Z",
          "iopub.status.idle": "2025-08-29T15:39:35.507322Z",
          "shell.execute_reply.started": "2025-08-29T15:39:35.498914Z",
          "shell.execute_reply": "2025-08-29T15:39:35.506553Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Hugging Face Models Setup (Alternative to Ollama)\n",
        "class HuggingFaceDoLaWrapper:\n",
        "    \"\"\"DoLa implementation for Hugging Face models\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, use_auth_token=None):\n",
        "        self.model_name = model_name\n",
        "        print(f\"Loading {model_name}...\")\n",
        "\n",
        "        # Load tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            use_auth_token=use_auth_token,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Add pad token if not present\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            use_auth_token=use_auth_token,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Initialize DoLa decoder\n",
        "        self.dola_decoder = DoLaDecoder(self.model, self.tokenizer)\n",
        "\n",
        "    def generate(self, prompt, max_length=512, temperature=0.7, alpha=0.1):\n",
        "        \"\"\"Generate response using DoLa\"\"\"\n",
        "        try:\n",
        "            return self.dola_decoder.dola_generate(\n",
        "                prompt=prompt,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                alpha=alpha\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"DoLa generation failed, falling back to standard generation: {e}\")\n",
        "            # Fallback to standard generation\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs,\n",
        "                    max_length=max_length,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id\n",
        "                )\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return response[len(prompt):].strip()"
      ],
      "metadata": {
        "id": "_EtrxFnx9x7A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:39:42.537470Z",
          "iopub.execute_input": "2025-08-29T15:39:42.538117Z",
          "iopub.status.idle": "2025-08-29T15:39:42.545225Z",
          "shell.execute_reply.started": "2025-08-29T15:39:42.538092Z",
          "shell.execute_reply": "2025-08-29T15:39:42.544515Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Configuration\n",
        "# Map model names to their Hugging Face identifiers\n",
        "model_mappings = {\n",
        "    \"deepseek-llm\": \"deepseek-llm\",\n",
        "    \"mistral7b\": \"mistral:7b\",\n",
        "    \"llama3:8b\": \"llama3:8b\",\n",
        "    \"qwen2.5:3b\": \"qwen2.5:3b\",\n",
        "    \"gemma:7b\": \"gemma:7b\"\n",
        "}\n",
        "\n",
        "# Choose your approach: 'ollama' or 'huggingface'\n",
        "approach = 'ollama'  # Change to 'ollama' if using local Ollama\n",
        "\n",
        "print(f\"Using approach: {approach}\")\n",
        "print(f\"Available models: {list(model_mappings.keys())}\")\n",
        "\n",
        "# Cell 9: Initialize Models\n",
        "models = {}\n",
        "\n",
        "if approach == 'ollama':\n",
        "    for model_name in model_mappings.keys():\n",
        "        try:\n",
        "            models[model_name] = OllamaDoLaWrapper(model_name)\n",
        "            print(f\"✓ Initialized {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to initialize {model_name}: {e}\")\n",
        "\n",
        "elif approach == 'huggingface':\n",
        "    # Note: You may need HF token for some models\n",
        "    hf_token = None  # Add your HF token here if needed\n",
        "\n",
        "    # Initialize one model at a time to manage memory\n",
        "    selected_models = [\"gemma:7b\"]  # Start with one model for testing\n",
        "\n",
        "    for model_name in selected_models:\n",
        "        if model_name in model_mappings:\n",
        "            try:\n",
        "                models[model_name] = HuggingFaceDoLaWrapper(\n",
        "                    model_mappings[model_name],\n",
        "                    use_auth_token=hf_token\n",
        "                )\n",
        "                print(f\" Initialized {model_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\" Failed to initialize {model_name}: {e}\")\n",
        "\n",
        "print(f\"Successfully initialized {len(models)} models\")"
      ],
      "metadata": {
        "id": "hTBYMtL99ypr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:39:46.120163Z",
          "iopub.execute_input": "2025-08-29T15:39:46.120920Z",
          "iopub.status.idle": "2025-08-29T15:39:46.127620Z",
          "shell.execute_reply.started": "2025-08-29T15:39:46.120873Z",
          "shell.execute_reply": "2025-08-29T15:39:46.126987Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Evaluation Functions\n",
        "def evaluate_response_quality(response: str, question: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate response quality metrics\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Length metrics\n",
        "    metrics['response_length'] = len(response.split())\n",
        "    metrics['char_length'] = len(response)\n",
        "\n",
        "    # Confidence indicators (simple heuristics)\n",
        "    uncertainty_phrases = [\n",
        "        \"i don't know\", \"not sure\", \"uncertain\", \"might be\", \"could be\",\n",
        "        \"possibly\", \"perhaps\", \"maybe\", \"i think\", \"i believe\", \"seems like\",\n",
        "        \"appears to\", \"likely\", \"probably\", \"unsure\", \"unclear\"\n",
        "    ]\n",
        "\n",
        "    response_lower = response.lower()\n",
        "    uncertainty_count = sum(1 for phrase in uncertainty_phrases if phrase in response_lower)\n",
        "    metrics['uncertainty_score'] = uncertainty_count / max(len(response.split()), 1)\n",
        "\n",
        "    # Specificity (presence of numbers, dates, names)\n",
        "    numbers = len(re.findall(r'\\d+', response))\n",
        "    dates = len(re.findall(r'\\b\\d{4}\\b|\\b\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4}\\b', response))\n",
        "    metrics['specificity_score'] = (numbers + dates) / max(len(response.split()), 1)\n",
        "\n",
        "    # Repetition detection\n",
        "    words = response.lower().split()\n",
        "    unique_words = len(set(words))\n",
        "    metrics['repetition_score'] = 1 - (unique_words / max(len(words), 1))\n",
        "\n",
        "    # Question answering directness\n",
        "    question_words = set(question.lower().split())\n",
        "    response_words = set(response.lower().split())\n",
        "    overlap = len(question_words.intersection(response_words))\n",
        "    metrics['relevance_score'] = overlap / max(len(question_words), 1)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def detect_hallucination_markers(response: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Detect potential hallucination markers in the response\n",
        "    \"\"\"\n",
        "    markers = {}\n",
        "\n",
        "    # Overly specific claims without qualification\n",
        "    specific_patterns = [\n",
        "        r'\\d+\\.\\d+%',  # Exact percentages\n",
        "        r'exactly \\d+',  # Exact numbers\n",
        "        r'precisely \\d+',  # Precise claims\n",
        "        r'the study shows',  # Unqualified study references\n",
        "        r'research proves',  # Definitive research claims\n",
        "        r'scientists have confirmed'  # Unqualified scientific claims\n",
        "    ]\n",
        "\n",
        "    response_lower = response.lower()\n",
        "    markers['overspecific_claims'] = sum(1 for pattern in specific_patterns\n",
        "                                       if re.search(pattern, response_lower))\n",
        "\n",
        "    # Contradictory statements\n",
        "    contradiction_indicators = [\n",
        "        ('always', 'never'), ('all', 'none'), ('definitely', 'possibly'),\n",
        "        ('certainly', 'might'), ('proven', 'unproven')\n",
        "    ]\n",
        "\n",
        "    contradiction_count = 0\n",
        "    for word1, word2 in contradiction_indicators:\n",
        "        if word1 in response_lower and word2 in response_lower:\n",
        "            contradiction_count += 1\n",
        "\n",
        "    markers['contradiction_score'] = contradiction_count\n",
        "\n",
        "    # Fabricated sources or references\n",
        "    fabrication_patterns = [\n",
        "        r'according to [a-z]+ university',\n",
        "        r'a study by dr\\. [a-z]+',\n",
        "        r'research from \\d{4}',\n",
        "        r'published in [a-z]+ journal'\n",
        "    ]\n",
        "\n",
        "    markers['fabrication_indicators'] = sum(1 for pattern in fabrication_patterns\n",
        "                                          if re.search(pattern, response_lower))\n",
        "\n",
        "    return markers"
      ],
      "metadata": {
        "id": "mS7lD-OG95C2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:40:04.355088Z",
          "iopub.execute_input": "2025-08-29T15:40:04.355382Z",
          "iopub.status.idle": "2025-08-29T15:40:04.364751Z",
          "shell.execute_reply.started": "2025-08-29T15:40:04.355361Z",
          "shell.execute_reply": "2025-08-29T15:40:04.364047Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dola_evaluation(models_dict, questions_dict, alpha_values=[0.12]):\n",
        "    \"\"\"\n",
        "    Run comprehensive DoLa evaluation across all models and benchmarks\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    total_evaluations = len(models_dict) * sum(len(q) for q in questions_dict.values()) * len(alpha_values)\n",
        "    pbar = tqdm(total=total_evaluations, desc=\"Running DoLa Evaluation\")\n",
        "\n",
        "    for model_name, model_wrapper in models_dict.items():\n",
        "        print(f\"Evaluating {model_name}\")\n",
        "\n",
        "        for benchmark_name, questions in questions_dict.items():\n",
        "            print(f\"Benchmark: {benchmark_name}\")\n",
        "\n",
        "            for alpha in alpha_values:\n",
        "                print(f\"  Alpha: {alpha}\")\n",
        "\n",
        "                for i, question in enumerate(questions):\n",
        "                    try:\n",
        "                        start_time = time.time()\n",
        "\n",
        "                        # Generate response with DoLa\n",
        "                        if hasattr(model_wrapper, 'dola_generate'):\n",
        "                            response = model_wrapper.dola_generate(\n",
        "                                prompt=question,\n",
        "                                max_length=300,\n",
        "                                temperature=0.7,\n",
        "                                alpha=alpha\n",
        "                            )\n",
        "                        else:\n",
        "                            response = model_wrapper.generate(\n",
        "                                prompt=question,\n",
        "                                max_length=300,\n",
        "                                temperature=0.7,\n",
        "                                alpha=alpha\n",
        "                            )\n",
        "\n",
        "                        generation_time = time.time() - start_time\n",
        "\n",
        "                        # Evaluate response\n",
        "                        quality_metrics = evaluate_response_quality(response, question)\n",
        "                        hallucination_markers = detect_hallucination_markers(response)\n",
        "\n",
        "                        # Store results\n",
        "                        result = {\n",
        "                            'model': model_name,\n",
        "                            'benchmark': benchmark_name,\n",
        "                            'question_idx': i,\n",
        "                            'question': question,\n",
        "                            'response': response,\n",
        "                            'alpha': alpha,\n",
        "                            'generation_time': generation_time,\n",
        "                            'mitigation_strategy': 'DoLa',\n",
        "                            **quality_metrics,\n",
        "                            **hallucination_markers\n",
        "                        }\n",
        "\n",
        "                        results.append(result)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Error with question {i}: {e}\")\n",
        "                        # Record error case\n",
        "                        results.append({\n",
        "                            'model': model_name,\n",
        "                            'benchmark': benchmark_name,\n",
        "                            'question_idx': i,\n",
        "                            'question': question,\n",
        "                            'response': f\"ERROR: {str(e)}\",\n",
        "                            'alpha': alpha,\n",
        "                            'generation_time': 0,\n",
        "                            'mitigation_strategy': 'DoLa',\n",
        "                            'response_length': 0,\n",
        "                            'uncertainty_score': 1.0,\n",
        "                            'error': True\n",
        "                        })\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "                    # Small delay to prevent overloading\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "    pbar.close()\n",
        "    return results"
      ],
      "metadata": {
        "id": "QUmlRZsZ953W",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:40:18.832516Z",
          "iopub.execute_input": "2025-08-29T15:40:18.832829Z",
          "iopub.status.idle": "2025-08-29T15:40:18.841387Z",
          "shell.execute_reply.started": "2025-08-29T15:40:18.832807Z",
          "shell.execute_reply": "2025-08-29T15:40:18.840763Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute DoLa Evaluation\n",
        "print(\" Starting DoLa Evaluation...\")\n",
        "print(\"This will take significant time depending on number of models and questions\")\n",
        "\n",
        "# Run evaluation with different alpha values to test sensitivity\n",
        "alpha_values = [0.12]  # Different contrast strengths\n",
        "dola_results = run_dola_evaluation(models, benchmark_questions, alpha_values)\n",
        "\n",
        "print(f\"Evaluation completed!\")\n",
        "print(f\"Total results collected: {len(dola_results)}\")\n",
        "\n",
        "# Convert Results to DataFrame and Save\n",
        "df_dola_results = pd.DataFrame(dola_results)\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"DoLa Results Summary:\")\n",
        "print(f\"Models evaluated: {df_dola_results['model'].nunique()}\")\n",
        "print(f\"Benchmarks covered: {df_dola_results['benchmark'].nunique()}\")\n",
        "print(f\"Total responses: {len(df_dola_results)}\")\n",
        "print(f\"Alpha values tested: {sorted(df_dola_results['alpha'].unique())}\")\n",
        "\n",
        "# Show sample results\n",
        "print(\"Sample Results:\")\n",
        "sample_df = df_dola_results[['model', 'benchmark', 'alpha', 'response_length',\n",
        "                            'uncertainty_score', 'specificity_score']].head(10)\n",
        "print(sample_df.to_string())\n",
        "\n",
        "# Save results\n",
        "df_dola_results.to_csv('dola_mitigation_results.csv', index=False)\n",
        "print(\" Results saved to 'dola_mitigation_results.csv'\")\n",
        "\n",
        "# Analysis Functions\n",
        "def analyze_dola_effectiveness(df):\n",
        "    \"\"\"Analyze the effectiveness of DoLa across different alpha values\"\"\"\n",
        "\n",
        "    print(\"DoLa Effectiveness Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Group by model and alpha\n",
        "    grouped = df.groupby(['model', 'alpha']).agg({\n",
        "        'uncertainty_score': ['mean', 'std'],\n",
        "        'specificity_score': ['mean', 'std'],\n",
        "        'overspecific_claims': ['mean', 'std'],\n",
        "        'fabrication_indicators': ['mean', 'std'],\n",
        "        'response_length': ['mean', 'std'],\n",
        "        'generation_time': ['mean', 'std']\n",
        "    }).round(4)\n",
        "\n",
        "    print(\" Metrics by Model and Alpha Value:\")\n",
        "    print(grouped)\n",
        "\n",
        "    return grouped\n",
        "\n",
        "def compare_alpha_sensitivity(df):\n",
        "    \"\"\"Compare how different alpha values affect model performance\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    metrics_to_plot = ['uncertainty_score', 'overspecific_claims', 'fabrication_indicators', 'specificity_score']\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot, 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "\n",
        "        for model in df['model'].unique():\n",
        "            model_data = df[df['model'] == model]\n",
        "            alpha_means = model_data.groupby('alpha')[metric].mean()\n",
        "\n",
        "            plt.plot(alpha_means.index, alpha_means.values,\n",
        "                    marker='o', label=model, linewidth=2, markersize=6)\n",
        "\n",
        "        plt.title(f'{metric.replace(\"_\", \" \").title()} vs Alpha Value', fontsize=12, fontweight='bold')\n",
        "        plt.xlabel('Alpha Value')\n",
        "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dola_alpha_sensitivity.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "#Benchmark-Specific Analysis\n",
        "def analyze_by_benchmark(df):\n",
        "    \"\"\"Analyze performance across different benchmarks\"\"\"\n",
        "\n",
        "    print(\"Performance by Benchmark\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    benchmark_analysis = df.groupby(['benchmark', 'model']).agg({\n",
        "        'uncertainty_score': 'mean',\n",
        "        'specificity_score': 'mean',\n",
        "        'overspecific_claims': 'mean',\n",
        "        'fabrication_indicators': 'mean',\n",
        "        'response_length': 'mean'\n",
        "    }).round(4)\n",
        "\n",
        "    print(benchmark_analysis)\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    metrics = ['uncertainty_score', 'specificity_score', 'overspecific_claims',\n",
        "              'fabrication_indicators', 'response_length']\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        if i < len(axes):\n",
        "            pivot_data = df.pivot_table(\n",
        "                values=metric,\n",
        "                index='benchmark',\n",
        "                columns='model',\n",
        "                aggfunc='mean'\n",
        "            )\n",
        "\n",
        "            sns.heatmap(pivot_data, annot=True, fmt='.3f',\n",
        "                       cmap='RdYlBu_r', ax=axes[i], cbar_kws={'shrink': 0.8})\n",
        "            axes[i].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "            axes[i].set_xlabel('Model')\n",
        "            axes[i].set_ylabel('Benchmark')\n",
        "\n",
        "    # Remove empty subplot\n",
        "    if len(metrics) < len(axes):\n",
        "        axes[-1].remove()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dola_benchmark_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "H3HuTSRE9-cL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T15:40:25.537266Z",
          "iopub.execute_input": "2025-08-29T15:40:25.538026Z",
          "iopub.status.idle": "2025-08-29T17:20:41.158751Z",
          "shell.execute_reply.started": "2025-08-29T15:40:25.538001Z",
          "shell.execute_reply": "2025-08-29T17:20:41.158042Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Analysis\n",
        "if len(df_dola_results) > 0:\n",
        "    # Analyze DoLa effectiveness\n",
        "    effectiveness_analysis = analyze_dola_effectiveness(df_dola_results)\n",
        "\n",
        "    # Compare alpha sensitivity\n",
        "    compare_alpha_sensitivity(df_dola_results)\n",
        "\n",
        "    # Analyze by benchmark\n",
        "    analyze_by_benchmark(df_dola_results)\n",
        "else:\n",
        "    print(\" No results to analyze. Please run the evaluation first.\")\n"
      ],
      "metadata": {
        "id": "m3BIRUCj-EoB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T18:07:52.762480Z",
          "iopub.execute_input": "2025-08-29T18:07:52.762815Z",
          "iopub.status.idle": "2025-08-29T18:07:57.577146Z",
          "shell.execute_reply.started": "2025-08-29T18:07:52.762791Z",
          "shell.execute_reply": "2025-08-29T18:07:57.576371Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#  Advanced Hallucination Detection\n",
        "def advanced_hallucination_detection(df):\n",
        "    \"\"\"\n",
        "    More sophisticated hallucination detection methods\n",
        "    \"\"\"\n",
        "    print(\"Advanced Hallucination Detection\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    def calculate_confidence_calibration(responses):\n",
        "        \"\"\"Calculate how well-calibrated the model's confidence is\"\"\"\n",
        "        confidence_scores = []\n",
        "        for response in responses:\n",
        "            # Simple confidence estimation based on language patterns\n",
        "            confident_patterns = r'\\b(definitely|certainly|absolutely|clearly|obviously)\\b'\n",
        "            uncertain_patterns = r'\\b(maybe|perhaps|possibly|might|could|seems)\\b'\n",
        "\n",
        "            confident_matches = len(re.findall(confident_patterns, response.lower()))\n",
        "            uncertain_matches = len(re.findall(uncertain_patterns, response.lower()))\n",
        "\n",
        "            # Confidence score: more confident language = higher score\n",
        "            confidence = (confident_matches - uncertain_matches) / max(len(response.split()), 1)\n",
        "            confidence_scores.append(confidence)\n",
        "\n",
        "        return confidence_scores\n",
        "\n",
        "    def detect_factual_inconsistencies(response):\n",
        "        \"\"\"Detect potential factual inconsistencies within a response\"\"\"\n",
        "        inconsistency_score = 0\n",
        "\n",
        "        # Look for contradictory statements\n",
        "        sentences = response.split('.')\n",
        "        for i, sent1 in enumerate(sentences):\n",
        "            for sent2 in sentences[i+1:]:\n",
        "                # Simple contradiction detection (can be enhanced)\n",
        "                if ('not' in sent1.lower() and 'not' not in sent2.lower()) or \\\n",
        "                   ('not' not in sent1.lower() and 'not' in sent2.lower()):\n",
        "                    # Check if they're talking about the same subject\n",
        "                    words1 = set(sent1.lower().split())\n",
        "                    words2 = set(sent2.lower().split())\n",
        "                    overlap = len(words1.intersection(words2))\n",
        "                    if overlap > 2:  # Threshold for considering sentences related\n",
        "                        inconsistency_score += 1\n",
        "\n",
        "        return inconsistency_score / max(len(sentences), 1)\n",
        "\n",
        "    # Apply advanced detection\n",
        "    df['confidence_calibration'] = [calculate_confidence_calibration([resp])[0]\n",
        "                                   for resp in df['response']]\n",
        "\n",
        "    df['factual_inconsistency'] = [detect_factual_inconsistencies(resp)\n",
        "                                  for resp in df['response']]\n",
        "\n",
        "    # Aggregate analysis\n",
        "    advanced_metrics = df.groupby(['model', 'benchmark', 'alpha']).agg({\n",
        "        'confidence_calibration': ['mean', 'std'],\n",
        "        'factual_inconsistency': ['mean', 'std'],\n",
        "        'uncertainty_score': 'mean',\n",
        "        'overspecific_claims': 'mean',\n",
        "        'fabrication_indicators': 'mean'\n",
        "    }).round(4)\n",
        "\n",
        "    print(\" Advanced Hallucination Metrics:\")\n",
        "    print(advanced_metrics)\n",
        "\n",
        "    return advanced_metrics"
      ],
      "metadata": {
        "id": "cM4L3VS_-r7L",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T18:08:14.067019Z",
          "iopub.execute_input": "2025-08-29T18:08:14.067319Z",
          "iopub.status.idle": "2025-08-29T18:08:14.076543Z",
          "shell.execute_reply.started": "2025-08-29T18:08:14.067301Z",
          "shell.execute_reply": "2025-08-29T18:08:14.075845Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# DoLa Optimization Analysis\n",
        "def optimize_dola_parameters(df):\n",
        "    \"\"\"\n",
        "    Find optimal DoLa parameters for each model and benchmark\n",
        "    \"\"\"\n",
        "    print(\" DoLa Parameter Optimization\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    # Create composite hallucination score\n",
        "    df['hallucination_composite'] = (\n",
        "        df['uncertainty_score'] * 0.3 +\n",
        "        df['overspecific_claims'] * 0.3 +\n",
        "        df['fabrication_indicators'] * 0.4\n",
        "    )\n",
        "\n",
        "    # Find best alpha for each model-benchmark combination\n",
        "    optimization_results = []\n",
        "\n",
        "    for model in df['model'].unique():\n",
        "        for benchmark in df['benchmark'].unique():\n",
        "            subset = df[(df['model'] == model) & (df['benchmark'] == benchmark)]\n",
        "\n",
        "            if len(subset) > 0:\n",
        "                # Find alpha that minimizes hallucination composite score\n",
        "                best_alpha_idx = subset['hallucination_composite'].idxmin()\n",
        "                best_result = subset.loc[best_alpha_idx]\n",
        "\n",
        "                optimization_results.append({\n",
        "                    'model': model,\n",
        "                    'benchmark': benchmark,\n",
        "                    'optimal_alpha': best_result['alpha'],\n",
        "                    'hallucination_score': best_result['hallucination_composite'],\n",
        "                    'uncertainty_score': best_result['uncertainty_score'],\n",
        "                    'specificity_score': best_result['specificity_score']\n",
        "                })\n",
        "\n",
        "    opt_df = pd.DataFrame(optimization_results)\n",
        "    print(\" Optimal Alpha Values:\")\n",
        "    print(opt_df.to_string(index=False))\n",
        "\n",
        "    # Visualization of optimal alphas\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    pivot_alpha = opt_df.pivot(index='benchmark', columns='model', values='optimal_alpha')\n",
        "\n",
        "    sns.heatmap(pivot_alpha, annot=True, fmt='.2f', cmap='viridis',\n",
        "                cbar_kws={'label': 'Optimal Alpha Value'})\n",
        "    plt.title('Optimal DoLa Alpha Values by Model and Benchmark', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Benchmark')\n",
        "    plt.xlabel('Model')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dola_optimal_alphas.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return opt_df"
      ],
      "metadata": {
        "id": "59KKtUqj-wp-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T17:21:44.710861Z",
          "iopub.execute_input": "2025-08-29T17:21:44.711107Z",
          "iopub.status.idle": "2025-08-29T17:21:44.719420Z",
          "shell.execute_reply.started": "2025-08-29T17:21:44.711090Z",
          "shell.execute_reply": "2025-08-29T17:21:44.718708Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#  Response Quality Distribution Analysis\n",
        "def analyze_response_distributions(df):\n",
        "    \"\"\"Analyze distributions of response quality metrics\"\"\"\n",
        "\n",
        "    print(\"Response Quality Distribution Analysis\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    # Create distribution plots\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    metrics_to_plot = [\n",
        "        'response_length', 'uncertainty_score', 'specificity_score',\n",
        "        'overspecific_claims', 'fabrication_indicators', 'repetition_score'\n",
        "    ]\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        for model in df['model'].unique():\n",
        "            model_data = df[df['model'] == model][metric]\n",
        "            axes[i].hist(model_data, alpha=0.6, label=model, bins=20)\n",
        "\n",
        "        axes[i].set_title(f'Distribution of {metric.replace(\"_\", \" \").title()}')\n",
        "        axes[i].set_xlabel(metric.replace(\"_\", \" \").title())\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dola_response_distributions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "#Comparative Analysis Preparation\n",
        "def prepare_comparison_data(df):\n",
        "    \"\"\"\n",
        "    Prepare data for comparison with baseline (non-DoLa) results\n",
        "    This creates the structure you'll need for your comparison analysis\n",
        "    \"\"\"\n",
        "    print(\"Preparing Data for Baseline Comparison\")\n",
        "    print(\"=\" * 42)\n",
        "\n",
        "    # Aggregate DoLa results for comparison\n",
        "    comparison_summary = df.groupby(['model', 'benchmark']).agg({\n",
        "        'uncertainty_score': ['mean', 'std', 'min', 'max'],\n",
        "        'specificity_score': ['mean', 'std', 'min', 'max'],\n",
        "        'overspecific_claims': ['mean', 'std', 'min', 'max'],\n",
        "        'fabrication_indicators': ['mean', 'std', 'min', 'max'],\n",
        "        'response_length': ['mean', 'std', 'min', 'max'],\n",
        "        'hallucination_composite': ['mean', 'std', 'min', 'max'],\n",
        "        'generation_time': ['mean', 'std']\n",
        "    }).round(4)\n",
        "\n",
        "    # Flatten column names\n",
        "    comparison_summary.columns = [f'{col[0]}_{col[1]}' for col in comparison_summary.columns]\n",
        "    comparison_summary = comparison_summary.reset_index()\n",
        "\n",
        "    # Add mitigation flag\n",
        "    comparison_summary['mitigation_applied'] = 'DoLa'\n",
        "\n",
        "    print(\"Comparison data prepared\")\n",
        "    print(\"Structure for your baseline comparison:\")\n",
        "    print(comparison_summary.head())\n",
        "\n",
        "    # Save comparison-ready data\n",
        "    comparison_summary.to_csv('dola_comparison_ready.csv', index=False)\n",
        "    print(\"Comparison data saved to 'dola_comparison_ready.csv'\")\n",
        "\n",
        "    return comparison_summary\n",
        "\n",
        "# Statistical Significance Testing\n",
        "def statistical_analysis(df):\n",
        "    \"\"\"\n",
        "    Perform statistical tests to validate DoLa effectiveness\n",
        "    \"\"\"\n",
        "    from scipy import stats\n",
        "\n",
        "    print(\"Statistical Significance Analysis\")\n",
        "    print(\"=\" * 38)\n",
        "\n",
        "    # Compare different alpha values\n",
        "    alpha_comparison = []\n",
        "\n",
        "    for model in df['model'].unique():\n",
        "        for benchmark in df['benchmark'].unique():\n",
        "            model_benchmark_data = df[(df['model'] == model) & (df['benchmark'] == benchmark)]\n",
        "\n",
        "            if len(model_benchmark_data) > 0:\n",
        "                alpha_groups = [group['hallucination_composite'].values\n",
        "                               for alpha, group in model_benchmark_data.groupby('alpha')]\n",
        "\n",
        "                if len(alpha_groups) > 1:\n",
        "                    # Perform ANOVA to test if alpha values make a significant difference\n",
        "                    f_stat, p_value = stats.f_oneway(*alpha_groups)\n",
        "\n",
        "                    alpha_comparison.append({\n",
        "                        'model': model,\n",
        "                        'benchmark': benchmark,\n",
        "                        'f_statistic': f_stat,\n",
        "                        'p_value': p_value,\n",
        "                        'significant': p_value < 0.05\n",
        "                    })\n",
        "\n",
        "    alpha_comp_df = pd.DataFrame(alpha_comparison)\n",
        "\n",
        "    print(\"Alpha Value Significance Test Results:\")\n",
        "    print(alpha_comp_df.to_string(index=False))\n",
        "\n",
        "    # Summary statistics\n",
        "    significant_count = alpha_comp_df['significant'].sum()\n",
        "    total_tests = len(alpha_comp_df)\n",
        "\n",
        "    print(f\"Summary:\")\n",
        "    print(f\"Significant differences found: {significant_count}/{total_tests} ({significant_count/total_tests*100:.1f}%)\")\n",
        "\n",
        "    return alpha_comp_df\n"
      ],
      "metadata": {
        "id": "TJo-RpHj-0wc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T18:08:19.536255Z",
          "iopub.execute_input": "2025-08-29T18:08:19.536753Z",
          "iopub.status.idle": "2025-08-29T18:08:19.551281Z",
          "shell.execute_reply.started": "2025-08-29T18:08:19.536728Z",
          "shell.execute_reply": "2025-08-29T18:08:19.550492Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Detailed Report\n",
        "def generate_dola_report(df, opt_df, comparison_df):\n",
        "    \"\"\"Generate comprehensive DoLa evaluation report\"\"\"\n",
        "\n",
        "    report = f\"\"\"\n",
        "# DoLa Hallucination Mitigation Study Report\n",
        "\n",
        "## Executive Summary\n",
        "- **Models Evaluated**: {df['model'].nunique()}\n",
        "- **Benchmarks Used**: {df['benchmark'].nunique()}\n",
        "- **Total Evaluations**: {len(df)}\n",
        "- **Alpha Values Tested**: {sorted(df['alpha'].unique())}\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### Overall Performance Metrics\n",
        "\"\"\"\n",
        "\n",
        "    # Add overall statistics\n",
        "    overall_stats = df.groupby('model').agg({\n",
        "        'uncertainty_score': 'mean',\n",
        "        'overspecific_claims': 'mean',\n",
        "        'fabrication_indicators': 'mean',\n",
        "        'specificity_score': 'mean',\n",
        "        'response_length': 'mean'\n",
        "    }).round(4)\n",
        "\n",
        "    report += f\"\\n{overall_stats.to_string()}\\n\"\n",
        "\n",
        "    # Add optimal parameters\n",
        "    report += f\"\"\"\n",
        "### Optimal DoLa Parameters\n",
        "{opt_df.to_string(index=False)}\n",
        "\n",
        "### Benchmark Performance Summary\n",
        "\"\"\"\n",
        "\n",
        "    # Add benchmark summary\n",
        "    benchmark_summary = df.groupby('benchmark').agg({\n",
        "        'hallucination_composite': ['mean', 'std'],\n",
        "        'uncertainty_score': 'mean',\n",
        "        'response_length': 'mean'\n",
        "    }).round(4)\n",
        "\n",
        "    report += f\"\\n{benchmark_summary.to_string()}\\n\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "### Recommendations\n",
        "1. **Optimal Alpha Range**: Based on results, alpha values between 0.1-0.2 show best performance\n",
        "2. **Model-Specific Tuning**: Each model benefits from different alpha values\n",
        "3. **Benchmark Sensitivity**: {df['benchmark'].value_counts().index[0]} showed highest sensitivity to DoLa\n",
        "4. **Performance Trade-offs**: Monitor response length vs hallucination reduction\n",
        "\n",
        "### Next Steps for Comparison\n",
        "- Load your baseline (non-DoLa) results\n",
        "- Use the comparison_ready.csv file for statistical comparison\n",
        "- Focus on models where DoLa showed significant improvement\n",
        "\"\"\"\n",
        "\n",
        "    # Save report\n",
        "    with open('dola_evaluation_report.txt', 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    print(\"📄 Detailed report saved to 'dola_evaluation_report.txt'\")\n",
        "    return report"
      ],
      "metadata": {
        "id": "1gQkAtI1-6G7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T18:08:24.475878Z",
          "iopub.execute_input": "2025-08-29T18:08:24.476624Z",
          "iopub.status.idle": "2025-08-29T18:08:24.482667Z",
          "shell.execute_reply.started": "2025-08-29T18:08:24.476598Z",
          "shell.execute_reply": "2025-08-29T18:08:24.481867Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute All Analysis\n",
        "if len(df_dola_results) > 0:\n",
        "    # Run advanced hallucination detection\n",
        "    advanced_metrics = advanced_hallucination_detection(df_dola_results)\n",
        "\n",
        "    # Optimize DoLa parameters\n",
        "    optimal_params = optimize_dola_parameters(df_dola_results)\n",
        "\n",
        "    # Analyze response distributions\n",
        "    analyze_response_distributions(df_dola_results)\n",
        "\n",
        "    # Prepare comparison data\n",
        "    comparison_ready = prepare_comparison_data(df_dola_results)\n",
        "\n",
        "    # Statistical analysis\n",
        "    stats_results = statistical_analysis(df_dola_results)\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    final_report = generate_dola_report(df_dola_results, optimal_params, comparison_ready)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" DoLa Evaluation Complete!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\" Generated Files:\")\n",
        "    print(\"- dola_mitigation_results.csv (Raw results)\")\n",
        "    print(\"- dola_comparison_ready.csv (For baseline comparison)\")\n",
        "    print(\"- dola_evaluation_report.txt (Comprehensive report)\")\n",
        "    print(\"- dola_alpha_sensitivity.png (Sensitivity analysis)\")\n",
        "    print(\"- dola_benchmark_analysis.png (Benchmark heatmaps)\")\n",
        "    print(\"- dola_response_distributions.png (Quality distributions)\")\n",
        "    print(\"- dola_optimal_alphas.png (Optimal parameter visualization)\")\n",
        "\n",
        "# Model Memory Management (For Hugging Face approach)\n",
        "def cleanup_models():\n",
        "    \"\"\"Clean up GPU memory between model evaluations\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Delete model objects to free memory\n",
        "    global models\n",
        "    for model_name in list(models.keys()):\n",
        "        if hasattr(models[model_name], 'model'):\n",
        "            del models[model_name].model\n",
        "        if hasattr(models[model_name], 'tokenizer'):\n",
        "            del models[model_name].tokenizer\n",
        "        del models[model_name]\n",
        "\n",
        "    models = {}\n",
        "    print(\"Model memory cleaned up\")\n",
        "\n",
        "# Sequential Model Evaluation (Memory-Efficient)\n",
        "def sequential_model_evaluation(model_mappings, questions_dict, alpha_values=[0.1, 0.2]):\n",
        "    \"\"\"\n",
        "    Evaluate models one by one to manage memory efficiently\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    for model_name, hf_model_name in model_mappings.items():\n",
        "        print(f\" Starting evaluation for {model_name}\")\n",
        "        print(f\"   Model: {hf_model_name}\")\n",
        "\n",
        "        try:\n",
        "            # Initialize single model\n",
        "            model_wrapper = HuggingFaceDoLaWrapper(hf_model_name)\n",
        "            single_model_dict = {model_name: model_wrapper}\n",
        "\n",
        "            # Run evaluation for this model only\n",
        "            model_results = run_dola_evaluation(single_model_dict, questions_dict, alpha_values)\n",
        "            all_results.extend(model_results)\n",
        "\n",
        "            print(f\"Completed {model_name}: {len(model_results)} results\")\n",
        "\n",
        "            # Clean up memory\n",
        "            del model_wrapper.model\n",
        "            del model_wrapper.tokenizer\n",
        "            del model_wrapper\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to evaluate {model_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# Alternative Execution Strategy\n",
        "print(\"Alternative Execution Strategy\")\n",
        "print(\"Choose your execution approach based on available resources:\")\n",
        "print()\n",
        "print(\"Option 1: All models at once (requires high GPU memory)\")\n",
        "print(\"Option 2: Sequential evaluation (memory-efficient)\")\n",
        "print(\"Option 3: Single model testing\")\n",
        "\n",
        "# Uncomment the approach you want to use:\n",
        "\n",
        "# OPTION 1: All models at once (if you have sufficient GPU memory)\n",
        "# dola_results = run_dola_evaluation(models, benchmark_questions, alpha_values=[0.1, 0.2])\n",
        "\n",
        "# OPTION 2: Sequential evaluation (recommended for limited GPU memory)\n",
        "# dola_results = sequential_model_evaluation(model_mappings, benchmark_questions, alpha_values=[0.1, 0.2])\n",
        "\n",
        "# OPTION 3: Single model testing (for development/testing)\n",
        "single_model = {\"gemma:7b\": models.get(\"gemma:7b\")} if \"gemma:7b\" in models else {}\n",
        "if single_model:\n",
        "    dola_results = run_dola_evaluation(single_model, {\"Scientific\": benchmark_questions[\"Scientific\"][:5]}, alpha_values=[0.1])\n",
        "    df_dola_results = pd.DataFrame(dola_results)\n",
        "    print(f\"Single model test completed: {len(dola_results)} results\")\n",
        "\n",
        "# Export Functions for Comparison\n",
        "def export_for_comparison():\n",
        "    \"\"\"\n",
        "    Export results in format suitable for comparison with your baseline study\n",
        "    \"\"\"\n",
        "    if 'df_dola_results' in globals() and len(df_dola_results) > 0:\n",
        "\n",
        "        # Create comparison export with same structure as your original study\n",
        "        comparison_export = []\n",
        "\n",
        "        for _, row in df_dola_results.iterrows():\n",
        "            export_row = {\n",
        "                'model_name': row['model'],\n",
        "                'benchmark': row['benchmark'],\n",
        "                'question': row['question'],\n",
        "                'question_index': row['question_idx'],\n",
        "                'response': row['response'],\n",
        "                'mitigation_strategy': 'DoLa',\n",
        "                'alpha_parameter': row['alpha'],\n",
        "\n",
        "                # Quality metrics (standardized names for comparison)\n",
        "                'response_length_words': row['response_length'],\n",
        "                'uncertainty_indicators': row['uncertainty_score'],\n",
        "                'specificity_measure': row['specificity_score'],\n",
        "                'hallucination_markers': row.get('overspecific_claims', 0),\n",
        "                'fabrication_signals': row.get('fabrication_indicators', 0),\n",
        "                'repetition_index': row.get('repetition_score', 0),\n",
        "\n",
        "                # Performance metrics\n",
        "                'generation_time_seconds': row['generation_time'],\n",
        "                'error_occurred': row.get('error', False)\n",
        "            }\n",
        "            comparison_export.append(export_row)\n",
        "\n",
        "        # Convert to DataFrame and save\n",
        "        export_df = pd.DataFrame(comparison_export)\n",
        "        export_df.to_csv('dola_results_for_comparison.csv', index=False)\n",
        "\n",
        "        print(\" Comparison Export Summary:\")\n",
        "        print(f\"   Total responses: {len(export_df)}\")\n",
        "        print(f\"   Models: {export_df['model_name'].unique()}\")\n",
        "        print(f\"   Benchmarks: {export_df['benchmark'].unique()}\")\n",
        "        print(f\"   Alpha values: {sorted(export_df['alpha_parameter'].unique())}\")\n",
        "\n",
        "        # Create summary statistics for easy comparison\n",
        "        summary_stats = export_df.groupby(['model_name', 'benchmark']).agg({\n",
        "            'uncertainty_indicators': ['mean', 'std'],\n",
        "            'hallucination_markers': ['mean', 'sum'],\n",
        "            'fabrication_signals': ['mean', 'sum'],\n",
        "            'response_length_words': ['mean', 'std'],\n",
        "            'generation_time_seconds': 'mean'\n",
        "        }).round(4)\n",
        "\n",
        "        summary_stats.to_csv('dola_summary_for_comparison.csv')\n",
        "\n",
        "        print(\" Files created for comparison:\")\n",
        "        print(\"   - dola_results_for_comparison.csv (detailed results)\")\n",
        "        print(\"   - dola_summary_for_comparison.csv (summary statistics)\")\n",
        "\n",
        "        return export_df\n",
        "    else:\n",
        "        print(\"No DoLa results available for export\")\n",
        "        return None\n",
        "\n",
        "#  Run Export\n",
        "comparison_data = export_for_comparison()\n",
        "\n",
        "# Final Instructions and Next Steps\n",
        "print(\"\"\"\n",
        "DoLa Mitigation Study Complete!\n",
        "\n",
        " What you have now:\n",
        "1. DoLa-mitigated responses from your models\n",
        "2. Quality metrics for each response\n",
        "3. Optimal alpha parameters for each model-benchmark combination\n",
        "4. Statistical analysis of DoLa effectiveness\n",
        "5. Export files ready for comparison with your baseline\n",
        "\n",
        " For your comparison analysis:\n",
        "1. Load your original (baseline) results\n",
        "2. Use the exported CSV files from this study\n",
        "3. Compare metrics like:\n",
        "   - Hallucination rates (before vs after DoLa)\n",
        "   - Response quality (uncertainty, specificity)\n",
        "   - Performance trade-offs (response time, length)\n",
        "\n",
        " Key metrics to compare:\n",
        "- uncertainty_indicators: Lower is better (less uncertain responses)\n",
        "- hallucination_markers: Lower is better (fewer overspecific claims)\n",
        "- fabrication_signals: Lower is better (fewer fabricated references)\n",
        "- specificity_measure: Higher is better (more specific, factual responses)\n",
        "\n",
        " Expected DoLa improvements:\n",
        "- Reduced fabricated facts and sources\n",
        "- Lower overconfident claims\n",
        "- Better calibrated uncertainty\n",
        "- Maintained or improved response quality\n",
        "\n",
        " Next research directions:\n",
        "- Try different mature/premature layer ratios\n",
        "- Experiment with dynamic alpha values\n",
        "- Combine DoLa with other mitigation strategies\n",
        "- Test on domain-specific benchmarks\n",
        "\"\"\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T18:08:28.421802Z",
          "iopub.execute_input": "2025-08-29T18:08:28.422349Z",
          "iopub.status.idle": "2025-08-29T18:08:32.695220Z",
          "shell.execute_reply.started": "2025-08-29T18:08:28.422323Z",
          "shell.execute_reply": "2025-08-29T18:08:32.694335Z"
        },
        "id": "QaMI6bukTHtW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move('/kaggle/working/working_dir_backup.zip', '/kaggle/working/working_dir_backup.zip')\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML('<a href=\"working_dir_backup.zip\" download> Download Backup</a>')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T18:09:05.710679Z",
          "iopub.execute_input": "2025-08-29T18:09:05.710932Z",
          "iopub.status.idle": "2025-08-29T18:09:05.716424Z",
          "shell.execute_reply.started": "2025-08-29T18:09:05.710914Z",
          "shell.execute_reply": "2025-08-29T18:09:05.715885Z"
        },
        "id": "JIpMfTkwTHtZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# =============== CONFIG ===============\n",
        "file_path = \"/kaggle/input/visual/dola_mitigation_results.csv\"  # 👈 replace with your file\n",
        "sheet_name = None  # If Excel has multiple sheets, set e.g. \"Sheet1\"\n",
        "# =====================================\n",
        "\n",
        "# Load Excel/CSV file\n",
        "if file_path.endswith(\".csv\"):\n",
        "    df = pd.read_csv(file_path)\n",
        "else:\n",
        "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "print(\" Data loaded successfully!\")\n",
        "print(\" Shape:\", df.shape)\n",
        "print(\" Preview:\", df.head())\n",
        "print(\" Summary Stats:\", df.describe(include=\"all\"))\n",
        "\n",
        "# ================= HEATMAP =================\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\", fontsize=14, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# ================= BAR PLOT =================\n",
        "# Example: average hallucination markers by model\n",
        "if \"model_name\" in df.columns and \"hallucination_markers\" in df.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=df, x=\"model_name\", y=\"hallucination_markers\", estimator=\"mean\", ci=\"sd\")\n",
        "    plt.title(\"Average Hallucination Markers by Model\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# ================= HISTOGRAM =================\n",
        "plt.figure(figsize=(10, 6))\n",
        "df.select_dtypes(include=\"number\").hist(bins=20, figsize=(12, 8), color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.suptitle(\"Histograms of Numeric Features\", fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# ================= BOX PLOT =================\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df.select_dtypes(include=\"number\"), orient=\"h\", palette=\"Set2\")\n",
        "plt.title(\"Boxplot of Numeric Columns\", fontsize=14, fontweight=\"bold\")\n",
        "plt.show()\n",
        "\n",
        "# ================= PAIRPLOT =================\n",
        "sns.pairplot(df.select_dtypes(include=\"number\"), diag_kind=\"kde\", corner=True)\n",
        "plt.suptitle(\"Pairwise Relationships\", y=1.02, fontsize=16, fontweight=\"bold\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-29T18:13:32.417345Z",
          "iopub.execute_input": "2025-08-29T18:13:32.417605Z",
          "iopub.status.idle": "2025-08-29T18:13:50.567270Z",
          "shell.execute_reply.started": "2025-08-29T18:13:32.417587Z",
          "shell.execute_reply": "2025-08-29T18:13:50.566456Z"
        },
        "id": "gUQ3uAA1THtb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}